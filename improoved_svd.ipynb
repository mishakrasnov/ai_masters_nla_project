{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a738bf60-eeee-4c42-9372-add953c4ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from chronos import BaseChronosPipeline\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f1d9f1-52fa-4528-a20a-6401c7e3bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_linear(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(in_features = weights.shape[1],\n",
    "                               out_features =  weights.shape[0], bias = False)\n",
    "        self.layer.weight = weights\n",
    "        self.c = 0\n",
    "        self.auto_cor_matrix = torch.zeros(768, 768)\n",
    "        self.auto_cor_matrix = self.auto_cor_matrix.to('cuda')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.c += 1\n",
    "\n",
    "        tmp = x.view(-1,768)\n",
    "        \n",
    "        self.auto_cor_matrix += tmp.T @ tmp\n",
    "        \n",
    "        return self.layer(x)\n",
    "        \n",
    "def process_linear_layer(linear_layer):\n",
    "    weights = None\n",
    "    for p in linear_layer.parameters():\n",
    "            weights = p\n",
    "    return Custom_linear(weights) \n",
    "\n",
    "def process_model(model):\n",
    "    for i in range(12):\n",
    "        \n",
    "        model.encoder.block[i].layer[0].SelfAttention.q =\\\n",
    "        process_linear_layer(model.encoder.block[i].layer[0].SelfAttention.q)\n",
    "\n",
    "        model.encoder.block[i].layer[0].SelfAttention.k =\\\n",
    "        process_linear_layer(model.encoder.block[i].layer[0].SelfAttention.k)\n",
    "\n",
    "        model.encoder.block[i].layer[0].SelfAttention.v =\\\n",
    "        process_linear_layer(model.encoder.block[i].layer[0].SelfAttention.v)\n",
    "\n",
    "        model.encoder.block[i].layer[0].SelfAttention.o =\\\n",
    "        process_linear_layer(model.encoder.block[i].layer[0].SelfAttention.o)\n",
    "\n",
    "\n",
    "        model.decoder.block[i].layer[0].SelfAttention.q =\\\n",
    "        process_linear_layer(model.decoder.block[i].layer[0].SelfAttention.q)\n",
    "\n",
    "        model.decoder.block[i].layer[0].SelfAttention.k =\\\n",
    "        process_linear_layer(model.decoder.block[i].layer[0].SelfAttention.k)\n",
    "\n",
    "        model.decoder.block[i].layer[0].SelfAttention.v =\\\n",
    "        process_linear_layer(model.decoder.block[i].layer[0].SelfAttention.v)\n",
    "\n",
    "        model.decoder.block[i].layer[0].SelfAttention.o =\\\n",
    "        process_linear_layer(model.decoder.block[i].layer[0].SelfAttention.o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18b814bd-d1b2-45c0-9dcd-fc940186c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reduced_Linear(nn.Module):\n",
    "    def __init__(self, dense_linear_layer, rank = 1000, type = 'matrix'):\n",
    "        super(Reduced_Linear, self).__init__()\n",
    "        \n",
    "        matrix = None\n",
    "        \n",
    "        for p in dense_linear_layer.parameters():\n",
    "            matrix = p.detach()\n",
    "\n",
    "        if type == 'matrix':\n",
    "        \n",
    "            U, S, Vh = torch.linalg.svd(matrix.float(), full_matrices=False)\n",
    "            S = S[:rank]\n",
    "            U = U[:, :rank]\n",
    "            Vh = Vh[:rank,]\n",
    "            self.linear1 =  nn.Linear(in_features=matrix.shape[1], out_features=rank, bias=False)\n",
    "            self.linear2 =  nn.Linear(in_features=rank, out_features=matrix.shape[0], bias=False)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for p in self.linear1.parameters():\n",
    "                    p.copy_((torch.diag(S)@Vh))\n",
    "                \n",
    "                for p in self.linear2.parameters():\n",
    "                    p.copy_(U)\n",
    "                    \n",
    "            self.linear1 = self.linear1.to('cuda')\n",
    "            self.linear2 = self.linear2.to('cuda')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            R_x = dense_linear_layer.auto_cor_matrix.float().cpu().numpy() / dense_linear_layer.c\n",
    "            R_sq = scipy.linalg.sqrtm(R_x + np.diag(np.ones(768)/10))\n",
    "        \n",
    "            R_fract_inv = np.linalg.inv(R_sq)\n",
    "            \n",
    "            Q = R_sq @ matrix.float().cpu().numpy()\n",
    "            U, S, V = np.linalg.svd(Q)\n",
    "\n",
    "            self.linear1 =  nn.Linear(in_features=matrix.shape[1], out_features=rank, bias=False)\n",
    "            self.linear2 =  nn.Linear(in_features=rank, out_features=matrix.shape[0], bias=False)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p in self.linear1.parameters():\n",
    "                    p.copy_(\n",
    "                        torch.diag(torch.tensor(S[:rank])).float() @ torch.tensor(V[:rank,:]).float()\n",
    "                    )\n",
    "                \n",
    "                for p in self.linear2.parameters():\n",
    "                    p.copy_(\n",
    "                        torch.tensor(R_fract_inv @ U[:,:rank])\n",
    "                    )\n",
    "                    \n",
    "            self.linear1 = self.linear1.to('cuda')\n",
    "            self.linear2 = self.linear2.to('cuda')\n",
    "                \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a343bfe4-fd72-4ab9-8410-b41c2239c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_model(model, rank, type):\n",
    "    for i in range(0,12):\n",
    "        print(i)\n",
    "        model.encoder.block[i].layer[0].SelfAttention.q =\\\n",
    "        Reduced_Linear(model.encoder.block[i].layer[0].SelfAttention.q, rank, type)\n",
    "\n",
    "        model.encoder.block[i].layer[0].SelfAttention.k =\\\n",
    "        Reduced_Linear(model.encoder.block[i].layer[0].SelfAttention.k, rank, type)\n",
    "\n",
    "        model.encoder.block[i].layer[0].SelfAttention.v =\\\n",
    "        Reduced_Linear(model.encoder.block[i].layer[0].SelfAttention.v, rank, type)\n",
    "\n",
    "        model.encoder.block[i].layer[0].SelfAttention.o =\\\n",
    "        Reduced_Linear(model.encoder.block[i].layer[0].SelfAttention.o, rank, type)\n",
    "\n",
    "        '''\n",
    "        model.decoder.block[i].layer[0].SelfAttention.q =\\\n",
    "        Reduced_Linear(model.decoder.block[i].layer[0].SelfAttention.q, rank, type)\n",
    "\n",
    "        model.decoder.block[i].layer[0].SelfAttention.k =\\\n",
    "        Reduced_Linear(model.decoder.block[i].layer[0].SelfAttention.k, rank, type)\n",
    "\n",
    "        model.decoder.block[i].layer[0].SelfAttention.v =\\\n",
    "        Reduced_Linear(model.decoder.block[i].layer[0].SelfAttention.v, rank, type)\n",
    "\n",
    "        model.decoder.block[i].layer[0].SelfAttention.o =\\\n",
    "        Reduced_Linear(model.decoder.block[i].layer[0].SelfAttention.o, rank, type)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6329e5-be04-41f7-b822-fa972e75edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method(data_train,data_test, method, rank):\n",
    "    horizon = data_test.shape[1]\n",
    "\n",
    "    pipeline = BaseChronosPipeline.from_pretrained(\n",
    "            \"amazon/chronos-bolt-base\",\n",
    "            device_map=\"cuda\",  # use \"cpu\" for CPU inference and \"mps\" for Apple Silicon\n",
    "            torch_dtype=torch.bfloat16,\n",
    "                \n",
    "        )\n",
    "    process_model(pipeline.inner_model)\n",
    "    forecast = pipeline.predict(context=data_train.to('cuda'),\n",
    "                                prediction_length=horizon)\n",
    "\n",
    "    reduce_model(pipeline.inner_model, rank, method)\n",
    "    \n",
    "    pipeline.inner_model.float()\n",
    "    forecast = pipeline.predict(context=data_train.to('cuda'),\n",
    "                                prediction_length=horizon)\n",
    "    \n",
    "    forecast = forecast.float().cpu().numpy()[:,4]\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d71c740-cb21-449b-bcc2-b4292a9d6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = []\n",
    "data_test = []\n",
    "with open('taxi_data/taxi_30min/train/train.json', 'r') as f:\n",
    "    for l in f:\n",
    "        x = torch.tensor(json.loads(l)['target'])\n",
    "        cur_train = []\n",
    "        cur_test = []\n",
    "        #cur_train = [x[:512], x[512:1024]]\n",
    "        #cur_test = [x[512:512+10], x[1024:1024+10]]\n",
    "        for i in range(10):\n",
    "            cur_train.append(\n",
    "                x[i+10:i+10+512]\n",
    "            )\n",
    "            cur_test.append(\n",
    "                x[i+10+512:i+10+512+10]\n",
    "            )\n",
    "        data_train.append(torch.cat(cur_train).view(-1,512))\n",
    "        data_test.append(torch.cat(cur_test).view(-1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9ab1507-cff1-46d3-8a34-7826e2087c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torch.cat(data_train).view(-1, 512)\n",
    "data_test = torch.cat(data_test).view(-1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11c1bd01-19a3-419c-a26c-bd4a4872a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "f = evaluate_method(data_train, data_test, 'matrix', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8775d00c-874e-4a72-afdd-1be976370001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7865189"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sqrt(np.mean((data_test.numpy() - f)**2, axis = 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
